{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Personal Info:</b><br>\n",
    "<b>Name:</b> Ehud Michel<br>\n",
    "<b>ID:</b> 208952713\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Importing the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ehudmichel/Documents/studies/machine_learning/pythonProjectMatala\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ehudmichel/opt/anaconda3/lib/python3.8/site-packages/hebrew_tokenizer/tokenizer.py:121: FutureWarning: Possible nested set at position 729\n",
      "  self.scanner = re.compile(\n"
     ]
    }
   ],
   "source": [
    "# imports for reading and writing (input & output) files:\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import hebrew_tokenizer as hz\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Getting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = '.' + os.sep + 'input' + os.sep + 'annotated_corpus_for_train.xlsx'\n",
    "test_filename  = '.' + os.sep + 'input' + os.sep + 'corpus_for_test.xlsx'\n",
    "df_train = pd.read_excel(train_filename, 'corpus', index_col=None, na_values=['NA'])\n",
    "df_test  = pd.read_excel(test_filename,  'corpus', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>function that uses the hebrew tokenizer and generates:<br> \n",
    "    1.list with the counts of the group types for each story - \"num_grg_aper\"<br>\n",
    "    2.list with the most common tokens with thier counts of apperences - \"most countes_words\"<br>\n",
    "    3.set with all the group types - \"token_set\":<br> \n",
    "        ('HEBREW','PUNC','NUMBER',etc..)<br>\n",
    "    4.set with all the common types - \"all_words_set\"<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> function to inzilize values and new features to the manual tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_idexes_test(story_tokens,num_of_types_per_story):   \n",
    "    tokens_set = set()\n",
    "    all_words_set = set()\n",
    "    num_grg_aper = []\n",
    "    most_countes_words = []\n",
    "    for i in range(len(story_tokens)):\n",
    "        number_group_occur_dict = {}\n",
    "        number_words_occur_dict = {}\n",
    "        for group,token,token_num,(s_i,e_i) in story_tokens[i]:\n",
    "            if group in list(number_group_occur_dict.keys()):\n",
    "                number_group_occur_dict[group] += 1\n",
    "            else:\n",
    "                number_group_occur_dict[group] = 1\n",
    "            if group =='HEBREW':\n",
    "                if token in list(number_words_occur_dict.keys()):\n",
    "                    number_words_occur_dict[token] +=1\n",
    "                else:\n",
    "                    number_words_occur_dict[token] = 1\n",
    "        top_words_dict = dict(sorted(number_words_occur_dict.items(),key=itemgetter(1),reverse=True)[:num_of_types_per_story])\n",
    "        tokens_set.update(list(number_group_occur_dict.keys()))\n",
    "        all_words_set.update(list(top_words_dict.keys()))\n",
    "        num_grg_aper.append(number_group_occur_dict)\n",
    "        most_countes_words.append(top_words_dict)\n",
    "    return num_grg_aper, tokens_set, most_countes_words, all_words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inizilaing_vals_in_df_manual_test(df_train,num_of_types_per_story):\n",
    "    lang = ['HEBREW','ENGLISH']\n",
    "    df_train_copy = df_test.copy() # generating copy of the dataset\n",
    "    df_train_copy['tokenizer'] = df_train_copy.story.apply(lambda x: hz.tokenize(x,with_whitespaces=True)) #adding a \n",
    "    # tokeniazer column that contain tokenize object related to to story\n",
    "    \n",
    "    df_train_copy['len'] = df_train_copy['story'].apply(lambda x: len(x))#adding column name len that contains the length of each story\n",
    "    num_group_count_for_story, token_type_set, most_common_words, all_words = get_text_idexes_test(df_train_copy['tokenizer'],num_of_types_per_story)\n",
    "    #num_group_count_for_story - list of all the dictionaries that each contains counts of the toknizer groups apearences\n",
    "    #token_type_set            - set with all the possible token type ('HEBREW','PANCTUAION','DATE','NUMBER','WHITE_SPACE')\n",
    "    #most_common_words         - list that contains dictionary for each story with its nth(defiend in the summery function) number of most common words\n",
    "    #all_words                 - set with most common words from all the stories togather\n",
    "\n",
    "    for token in token_type_set:\n",
    "        df_train_copy.insert(loc=0,column=token,value=0) #inizilizng every cell in the tokens types colmuns with 0's\n",
    "    for word in all_words:\n",
    "        df_train_copy.insert(loc=len(token_type_set)+2,column=word,value=0) #adding column for each word in the all words set and inizilizng every cell in every word column with 0's\n",
    "    for i in range(df_train_copy.shape[0]): # for each row in the data\n",
    "        for group_type in list(num_group_count_for_story[i].keys()):# for each group type in the ith story\n",
    "            df_train_copy.loc[i, group_type] = num_group_count_for_story[i][group_type]\n",
    "        for type_ in list(most_common_words[i].keys()): # for each type in the list[i] of dictionary keys of most common types\n",
    "            df_train_copy.loc[i, type_] = most_common_words[i][type_]\n",
    "    df_train_copy = df_train_copy.drop(['story','tokenizer'],axis=1)\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='total',value=df_train_copy[token_type_set].sum(axis=1))\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='amount_of_words',value=df_train_copy[lang].sum(axis=1))\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='len_of_paragraph',value=df_train_copy['story'].apply(lambda x: len(x)))\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='len_of_paragraph_per_word',value=df_train_copy['len_of_paragraph'] / df_train_copy['amount_of_words'])\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='Len',value=df_train_copy['len'])\n",
    "    \n",
    "    df_train_copy.insert(loc=len(token_type_set),column='len_of_words',value=df_train_copy['Len'] / df_train_copy['amount_of_words'])\n",
    "\n",
    "    # adding amount of words in story column for each story\n",
    "    df_train_copy['len_of_words'] = df_train_copy['len'] / df_train_copy['amount_of_words'] # adding column with the number of tokens divide by number of types\n",
    "    del df_train_copy['len']\n",
    "    df_stats = df_train_copy.iloc[:,:13]\n",
    "    df_manual = df_train_copy.iloc[:,14:]\n",
    "    return df_stats,token_type_set,df_manual,all_words,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Function to print confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_conMatrix(y_test,y_pred):\n",
    "    print(f'Confusion Matrix:\\n {confusion_matrix(y_test,y_pred)}')\n",
    "      \n",
    "    print (f'Accuracy:\\n {round(accuracy_score(y_test,y_pred)*100, 4)}%')\n",
    "\n",
    "    print(f'Report:\\n {classification_report(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Function that sums the number of types occurnces in df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_sum(df_01):\n",
    "    df = df_01.copy()\n",
    "    dict_  = {}\n",
    "    for col in df.columns:\n",
    "        dict_[col] = df[col].sum()\n",
    "    dict_sorted = sorted(dict_.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    return dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats,stats_colmuns,df_manual,all_wrds_set = inizilaing_vals_in_df_manual(df_train,175)\n",
    "# עיבוד מידע על הדאטה בצורה ידנית"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_manual = type_sum(df_manual)\n",
    "\n",
    "df_manual_droped = df_manual.copy()\n",
    "col_to_drop = []\n",
    "for i,j in dict_df_manual:\n",
    "    if(j <=10):\n",
    "        col_to_drop.append(i)\n",
    "df_manual_droped = df_manual_droped.drop(columns=col_to_drop)\n",
    "df_manual_droped = df_manual_droped.copy()\n",
    "col_to_drop = []\n",
    "for i,j in dict_df_manual:\n",
    "    if(j >=58):\n",
    "        col_to_drop.append(i)\n",
    "df_manual_droped = df_manual_droped.drop(columns=col_to_drop)\n",
    "df_merge = pd.concat([df_stats,df_manual_droped],axis=1)\n",
    "X = df_merge\n",
    "y = df_train.gender\n",
    "y = y.apply(lambda x:0 if x=='m' else 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "clf_LG = LogisticRegression(random_state=0,max_iter=10000000).fit(X_train, y_train)\n",
    "y_pred_LG = clf_LG.predict(X_test)\n",
    "print_conMatrix(y_test=y_test,y_pred=y_pred_LG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LG = LogisticRegression(random_state=0,max_iter=10000000).fit(X, y)\n",
    "y_pred_LG = clf_LG.predict(X_test)\n",
    "print_conMatrix(y_test=y_test,y_pred=y_pred_LG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "      <th>test_example_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>לא הרבה יודעים אך אני מתעסק הרבה בשוק ההון ובמ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>כשהייתי בן 4 נולד לי אח קטן, אני חייב להודות ש...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>הכל התחיל אי שם בגיל ההתבגרות, היתה לי חברה מו...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>נסעתי עם חברים לים, היה יום חם ורצינו ללכת לנצ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>הייתי בכיתה י,  ואני והשכבה של יצאנו לטיול שנת...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story gender  test_example_id\n",
       "0    בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...      m              NaN\n",
       "1    לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...      m              NaN\n",
       "2    השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...      m              NaN\n",
       "3    לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...      m              NaN\n",
       "4    יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...      m              NaN\n",
       "..                                                 ...    ...              ...\n",
       "151  לא הרבה יודעים אך אני מתעסק הרבה בשוק ההון ובמ...    NaN            151.0\n",
       "152  כשהייתי בן 4 נולד לי אח קטן, אני חייב להודות ש...    NaN            152.0\n",
       "153  הכל התחיל אי שם בגיל ההתבגרות, היתה לי חברה מו...    NaN            153.0\n",
       "154  נסעתי עם חברים לים, היה יום חם ורצינו ללכת לנצ...    NaN            154.0\n",
       "155  הייתי בכיתה י,  ואני והשכבה של יצאנו לטיול שנת...    NaN            155.0\n",
       "\n",
       "[520 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dfs = pd.concat([df_train,df_test])\n",
    "all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_idexes(story_tokens,num_of_types_per_story):\n",
    "    \n",
    "    tokens_set = set()\n",
    "    all_words_set = set()\n",
    "    num_grg_aper = []\n",
    "    most_countes_words = []\n",
    "    for i in range(len(story_tokens)):\n",
    "        number_group_occur_dict = {}\n",
    "        number_words_occur_dict = {}\n",
    "        for group,token,token_num,(s_i,e_i) in story_tokens[i]:\n",
    "            if group in list(number_group_occur_dict.keys()):\n",
    "                number_group_occur_dict[group] += 1\n",
    "            else:\n",
    "                number_group_occur_dict[group] = 1\n",
    "            if group =='HEBREW':\n",
    "                if token in list(number_words_occur_dict.keys()):\n",
    "                    number_words_occur_dict[token] +=1\n",
    "                else:\n",
    "                    number_words_occur_dict[token] = 1\n",
    "        top_words_dict = dict(sorted(number_words_occur_dict.items(),key=itemgetter(1),reverse=True)[:num_of_types_per_story])\n",
    "        tokens_set.update(list(number_group_occur_dict.keys()))\n",
    "        all_words_set.update(list(top_words_dict.keys()))\n",
    "        num_grg_aper.append(number_group_occur_dict)\n",
    "        most_countes_words.append(top_words_dict)\n",
    "    return num_grg_aper, tokens_set, most_countes_words, all_words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inizilaing_vals_in_df_manual(all_dfs,num_of_types_per_story):\n",
    "    lang = ['HEBREW','ENGLISH']\n",
    "    df_train_copy = all_dfs.copy() # generating copy of the dataset\n",
    "    df_train_copy['tokenizer'] = df_train_copy.story.apply(lambda x: hz.tokenize(x,with_whitespaces=True)) #adding a \n",
    "    # tokeniazer column that contain tokenize object related to to story\n",
    "    \n",
    "    df_train_copy.gender = df_train_copy.gender.apply(lambda x:0 if x=='m' else 1) # modying m->0,f->1\n",
    "    df_train_copy['len'] = df_train_copy['story'].apply(lambda x: len(x))#adding column name len that contains the length of each story\n",
    "   \n",
    "    num_group_count_for_story, token_type_set, most_common_words, all_words = get_text_idexes(df_train_copy['tokenizer'],num_of_types_per_story)\n",
    "   \n",
    "    #num_group_count_for_story - list of all the dictionaries that each contains counts of the toknizer groups apearences\n",
    "    #token_type_set            - set with all the possible token type ('HEBREW','PANCTUAION','DATE','NUMBER','WHITE_SPACE')\n",
    "    #most_common_words         - list that contains dictionary for each story with its nth(defiend in the summery function) number of most common words\n",
    "    #all_words                 - set with most common words from all the stories togather\n",
    "\n",
    "    for token in token_type_set:\n",
    "        df_train_copy.insert(loc=0,column=token,value=0) #inizilizng every cell in the tokens types colmuns with 0's\n",
    "    for word in all_words:\n",
    "        df_train_copy.insert(loc=len(token_type_set)+2,column=word,value=0) #adding column for each word in the all words set and inizilizng every cell in every word column with 0's\n",
    "    for i in range(df_train_copy.shape[0]): # for each row in the data\n",
    "        for group_type in list(num_group_count_for_story[i].keys()):# for each group type in the ith story\n",
    "            df_train_copy.loc[i, group_type] = num_group_count_for_story[i][group_type]\n",
    "        for type_ in list(most_common_words[i].keys()): # for each type in the list[i] of dictionary keys of most common types\n",
    "            df_train_copy.loc[i, type_] = most_common_words[i][type_]\n",
    "    df_train_copy = df_train_copy.drop(['story','tokenizer'],axis=1)\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='total',value=df_train_copy[token_type_set].sum(axis=1))\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='amount_of_words',value=df_train_copy[lang].sum(axis=1))\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='len_of_paragraph',value=df_train['story'].apply(lambda x: len(x)))\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='len_of_paragraph_per_word',value=df_train_copy['len_of_paragraph'] / df_train_copy['amount_of_words'])\n",
    "    df_train_copy.insert(loc=len(token_type_set),column='Len',value=df_train_copy['len'])\n",
    "    \n",
    "    df_train_copy.insert(loc=len(token_type_set),column='len_of_words',value=df_train_copy['Len'] / df_train_copy['amount_of_words'])\n",
    "\n",
    "    # adding amount of words in story column for each story\n",
    "    df_train_copy['len_of_words'] = df_train_copy['len'] / df_train_copy['amount_of_words'] # adding column with the number of tokens divide by number of types\n",
    "    del df_train_copy['len']\n",
    "    df_stats = df_train_copy.iloc[:,:13]\n",
    "    df_manual = df_train_copy.iloc[:,14:]\n",
    "    return df_stats,token_type_set,df_manual,all_words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_df_manual_test = type_sum(df_manual_test)\n",
    "\n",
    "df_manual_droped_test = df_manual_test.copy()\n",
    "col_to_drop = []\n",
    "for i,j in dict_df_manual_test:\n",
    "    if(j <=10):\n",
    "        col_to_drop.append(i)\n",
    "df_manual_droped_test = df_manual_droped_test.drop(columns=col_to_drop)\n",
    "df_manual_droped_test = df_manual_droped_test.copy()\n",
    "col_to_drop = []\n",
    "for i,j in dict_df_manual_test:\n",
    "    if(j >=58):\n",
    "        col_to_drop.append(i)\n",
    "df_manual_droped_test = df_manual_droped_test.drop(columns=col_to_drop)\n",
    "df_merge_test = pd.concat([df_stats_test,df_manual_droped_test],axis=1)\n",
    "X_test_real = df_merge_test\n",
    "predicted_category = clf_LG.predict(X_test_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_test,stats_colmuns_test,df_manual_test,all_wrds_set_test = inizilaing_vals_in_df_manual_test(df_test,175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category = pd.Series(predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category = predicted_category.apply(lambda x:'m' if x==0 else 'f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example_id = df_test['test_example_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-60261a99987e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_stats_test\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mstats_colmuns_test\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdf_manual_test\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mall_wrds_set_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minizilaing_vals_in_df_manual\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_dfs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m175\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-23-1f4875976577>\u001B[0m in \u001B[0;36minizilaing_vals_in_df_manual\u001B[0;34m(all_dfs, num_of_types_per_story)\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mdf_train_copy\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'len'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_train_copy\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'story'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m#adding column name len that contains the length of each story\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m     \u001B[0mnum_group_count_for_story\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtoken_type_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmost_common_words\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mall_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_text_idexes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_train_copy\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'tokenizer'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_of_types_per_story\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;31m#num_group_count_for_story - list of all the dictionaries that each contains counts of the toknizer groups apearences\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-22-da43533013da>\u001B[0m in \u001B[0;36mget_text_idexes\u001B[0;34m(story_tokens, num_of_types_per_story)\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0mnumber_group_occur_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0mnumber_words_occur_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mgroup\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtoken_num\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms_i\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0me_i\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstory_tokens\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mgroup\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumber_group_occur_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m                 \u001B[0mnumber_group_occur_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mgroup\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "df_stats_test,stats_colmuns_test,df_manual_test,all_wrds_set_test = inizilaing_vals_in_df_manual(all_dfs,175)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'test_example_id':test_example_id,\n",
    "       'predicted_category': predicted_category}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_all,stats_colmuns_all,df_manual_all,all_wrds_set_all = inizilaing_vals_in_df_manual(all_dfs,130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}